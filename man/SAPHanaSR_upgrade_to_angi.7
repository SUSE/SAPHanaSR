.\" Version: 1.001 
.\"
.TH SAPHanaSR_upgrade_to_angi 7 "14 Feb 2024" "" "SAPHanaSR"
.\"
.SH NAME
SAPHanaSR_upgrade_to_angi \- How to upgrade from SAPHanaSR to SAPHanaSR-angi.
.PP
.\"
.SH DESCRIPTION
.PP
* What is the upgrade about?
.PP
SAPHanaSR-angi can be used to replace SAPHanaSR and SAPHanaSR-ScaleOut.
SAPHanaSR-angi is quite similar to SAPHanaSR and SAPHanaSR-ScaleOut, but not
fully backward compatible. Upgrading existing clusters is possible by following
a defined procedure.
.PP
The SAPHanaFilsystem RA can be used to improve resilience angainst NFS issue.
.PP
See REQUIREMENTS below and in manual pages SAPHanaSR(7) or SAPHanaSR-ScaleOut(7)
.
.PP
* What will be changed for SAP HANA scale-up scenarios?
.PP
.RS 2
a. The SAPHana RA and its multi-state config will be replaced by the new
SAPHanaController and its clone promotable config
.br
b. The SAPHanaSR.py HADR provider hook script will be replaced by the new
susHanaSR.py
.br
c. Node attributes will be removed
.br
hana_<sid>_vhost
hana_<sid>_site
hana_<sid>_remoteHost
lpa_<sid>_lpt
hana_<sid>_op_mode
hana_<sid>_srmode
TODO
.br
d. Site and global attributes will be removed from property SAPHanaSR
.br
hana_<sid>_glob_sync_state
TODO
.br
e. Site and global attributes will be added to property SAPHanaSR
.br
hana_<sid>_glob_topology
hana_<sid>_glob_prim
hana_<sid>_glob_sec
hana_<sid>_site_lpt_<site>
hana_<sid>_site_lss_<site>
hana_<sid>_site_mns_<site>
hana_<sid>_site_srr_<site>
hana_<sid>_site_opMode_<site>
hana_<sid>_site_srMode_<site>
hana_<sid>_site_srHook_<site>
hana_<sid>_site_srPoll_<site>
TODO
.RE
.PP
* What will be changed for SAP HANA scale-out scenarios?
.PP
.RS 2
a. The SAPHanaController RA and its multi-state config will be replaced by the
new SAPHanaController and its clone promotable config
.br
b. The SAPHanaSrMultiTarget.py HADR provider hook script will be replaced by
the new susHanaSR.py
.br
c. Node attributes will be removed
.br
gra
gsh
.br
d. Site and global attributes will be removed from property SAPHanaSR
.br
mts
upd
hana_<sid>_glob_sync_state
hana_<sid>_glob_srHook (in case of obsolete scale-out SAPHanaSR.py)
TODO
.br
e. Site and global attributes will be added to property SAPHanaSR
.br
hana_<sid>_glob_topology
hana_<sid>_site_lpt_<site>
hana_<sid>_site_lss_<site>
hana_<sid>_site_mns_<site>
hana_<sid>_site_srr_<site>
hana_<sid>_site_srMode_<site>
hana_<sid>_site_srPoll_<site>
TODO
.RE
.PP
* How does the  procedure look like at a glance?
.PP
.RS 2
1.1 Check for sane state of cluster, HANA and system replication
.br
1.2 Collect information, needed for upgrade
.br
1.3 Make backup of CIB, sudoers and global.ini
.br
2.1 Set SAPHana or SAPHanaController resource to maintenance
.br
2.2 Remove SAPHanaSR.py or SAPHanaSrMultiTarget.py from global.ini, HANA and sudoers
.br
2.3 Remove SAPHana or SAPHanaController resource config from CIB
.br
2.4 Remove SAPHanaSR property attributes from CIB
.br
2.5 Remove SAPHanaSR node attributes from CIB
.br
2.6 Remove SAPHanaSR or SAPHanaSR-ScaleOut RPM
.br
3.1 Install SAPHanaSR-angi RPM
.br
3.2 Add susHanaSR.py to sudoers, global.ini, HANA
.br
3.3 Add angi SAPHanaController resource config to CIB
.br
3.4 Refresh SAPHanaController resource and set it out of maintenance
.br
3.5 Add SAPHanaFilesystem resource (optional)
.br
4.1 Check for sane state of cluster, HANA and system replication
.br
4.2 Test RA on secondary and trigger susHanaSR.py (optional)
.RE
.PP
* What needs to be prepared upfront?
.PP
First make yourself familiar with concepts, components and configuration of
SAPHanaSR-angi. Refresh your knowledge of SAPHanaSR or SAPHanaSR-ScaleOut. 
.PP
Next the following information needs to be collected and documented before
upgrading a cluster:
.RS 2
1.1 Path to config backup directory at both sites
.br
1.2 Name of both cluster nodes, respectively both HANA master nameservers, see
SAPHanaSR-showAttr(8) 
.br
1.3 HANA SID and instance number, name of <sid>adm
.br
1.4 HANA virtual hostname, in case it is used
.br
1.5 Name and config of existing SAPHana, or SAPHanaController, resources and related constraints in CIB, see ocf_suse_SAPHana(7) or ocf_suse_SAPHanaController(7
.br
1.6 Path to sudoers permission config file and its content, e.g. /etc/sudoers.d/SAPHanaSR
.br
1.7 Name of existing SAPHanaSR.py section in global.ini and its content, see SAPHanaSR.py(7) and SAPHanaSR-manageProvider(8)
.br
2.1 Name and config for new SAPHanaController resources and related constraints, path to config template, see ocf_suse_SAPHanaController(7)
.br
2.2 Path to config template for new sudoers permission and its content, see
susHanaSR.py(7)
.br
2.3 Path to config template for new susHanaSR.py section, e.g. /usr/share/SAPHanaSR-angi/global.ini_susHanaSR, see susHanaSR.py(7) 
.br
2.4 Name and config for new SAPHanaFilesystem resources, path to config template
, see ocf_suse_SAPHanaFilesystem(7) (optional)
.RE
.PP
Finally prepare the config templates with correct values for the given cluster.
Ideally also the needed commands are prepared in detail.
.PP
.\"
.SH EXAMPLES
.PP
* Example for checking sane state of cluster, HANA and system replication.
.PP
This steps should be performed before doing anything with the cluster, and after
something has been done. Usually is done per Linux cluster. See also manual
pages SAPHanaSR_maintenance_examples(7), cs_show_saphanasr_status(8) and
section REQUIREMENTS below. For scale-out, SAPHanaSR-manageAttr(8) might be
helpful as well.
.PP
.RS 2
# cs_clusterstate -i
.br
# crm_mon -1r
.br
# crm configure show | grep cli-
.br
# SAPHanaSR-showAttr
.br
# cs_clusterstate -i
.RE
.PP
* Example for showing SID and instance number of SAP HANA.
.PP
The installed SAP HANA instance is shown (should be only one) with its SID and
instance number. For systemd-enabled HANA the same info can be fetched from
systemd. Needs to be done at least once per Linux cluster. See also manual page
SAPHanaSR_basic_cluster(7).
.PP
.RS 2
# /usr/sap/hostctrl/exe/saphostctrl -function ListInstances
.br
# systemd-cgls -u SAP.slice
.RE
.PP
* Example for collecting information on SAPHana resource config.
.PP
The names for SAPHana primitive and multi-state resource are determined, as
well as for related oder and (co-)location constraints. The SAPHana primitive
configuration is shown. Might be useful to see if there is anything special.
Needs to be done once per Linux cluster.
.PP
.RS 2
# crm_mon -1r
.br
# crm configure show |\\
.br
grep -e "[primitive|master|order|location].*SAPHana_"
.br
# crm configure show rsc_SAPHana_HA1_HDB00
.RE
.PP
* Example for removing SAPHana resource config from CIB.
.PP
First the CIB is written to file for backup.
Next the cluster is told to not stop orphaned resources and the SAPHana
multi-state resource is set into maintenance. Next the order and colocation
constraints are removed, the SAPHana multi-state resource is removed and the
orphaned primitive is refreshed. Then the cluster is told to stop orphaned
resources. Finally the resulting cluster state is shown. Needs to be done once
per Linux cluster. SID is HA1, Instance Number is 00.
The resource names have been determined as shown in the example above.
.br
Of course the CIB should be checked to see if the removal was successful. See
example above.
.PP
.RS 2
# crm configure show >cib.SAPHanaSR-backup
.br
# echo "property cib-bootstrap-options: stop-orphan-resources=false" |\\
  crm configure load update -
.br
# crm resource maintenance msl_SAPHana_HA1_HDB00 on
.br
# cibadmin --delete --xpath "//rsc_order[@id='ord_SAPHana_HA1_HDB00']"
.br
# cibadmin --delete --xpath "//rsc_colocation[@id='col_saphana_ip_HA1_HDB00']"
.br
# cibadmin --delete --xpath "//master[@id='msl_SAPHana_HA1_HDB00']"
.br
# crm resource refresh rsc_SAPHana_HA1_HDB00
.br
# echo "property cib-bootstrap-options: stop-orphan-resources=true" |\\
  crm configure load update -
.br
# crm_mon -1r
.RE
.PP
* Example for removing node attributes from CIB.
.PP
Needs to be done for both nodes, or both master nameservers.
Node is node1.
.PP
.RS 2
# crm configure show node1
.br
# crm_attribute -N ${attr_node} -G -n \"$attr_name\" -l $attr_store -q"
.RE
.PP
* Example for removing all SAPHanaSR property attributes from CIB.
.PP
All attributes of porperty SAPHanaSR will be removed. Needed attributes are
expected to be re-added by the RAs later. The attribute for srHook will be
added by the susHanaSR.py HADR provider script and might be missing until the
HANA sytem replication status changes. Needs to be done once per Linux cluster.
See also SAPHanaSR-showAttr(8) and SAPHanaSR.py(7) or SAPHanaSrMultiTarget.py(7)
respectively.
.PP
.RS 2
# crm configure show SAPHanaSR
.br
# crm configure show SAPHanaSR |\\
.br
awk -F"=" '$1~/hana_/ {print $1}' |\\
.br
while read; do crm_attribute --delete --type crm_config --name $REPLY; done
.RE
.PP
* Example for removing the SAPHanaSR.py hook script from global.ini and HANA.
.PP
The global.ini is copied for backup. Next the exact name (upper/lower case) of
the section is determined from global.ini. Then the currenct HADR provider
section is shown. If the section is identical with the shipped template, it can
be removed easily from the configuration. Finally the HADR provider hook script 
is removed from running HANA. Needs to be done for each HANA site.
SID is HA1, case sensitive HADR provider name is SAPHanaSR. See manual page
SAPHanaSR.py(7) or SAPHanaSrMultiTarget.py(7) for details on checking the hook
script integration.
.PP
.RS 2
# su - ha1adm
.br
~> cdcoc
.br
~> cp global.ini global.ini.SAPHanaSR-backup
.br
~> grep -i ha_dr_provider_saphanasr global.ini
.br
~> /usr/bin/SAPHanaSR-manageProvider --sid=HA1 --show --provider=SAPHanaSR
.br
~> /usr/bin/SAPHanaSR-manageProvider --sid=HA1 --reconfigure \\
.br
--remove /usr/share/SAPHanaSR/samples/global.ini
.br
~> hdbnsutil -reloadHADRProviders
.RE
.PP
* Example for removing the SAPHanaSR.py hook script from sudoers. 
.PP
Needs to be done on each node.
See manual page SAPHanaSR.py(7) for details on checking the hook script
integration.
.PP
.RS 2
# cp $SUDOER "$SUDOER".angi-bak
.br
# grep -v "$sidadm.*ALL..NOPASSWD.*crm_attribute.*$sid" "$SUDOER".angi-bak >$SUDOER
.RE
.PP
* Example for removing the SAPHanaSR package.
.PP
The package SAPHanaSR is removed from all cluster nodes. Related packages
defined by patterns and dependencies are not touched. Needs to be done once per
Linux cluster.
.PP
.RS 2
# crm cluster run "rpm -E --force SAPHanaSR"
.RE
.PP
* Example for installing the SAPHanaSR-angi package.
.PP
The package SAPHanaSR is installed on all cluster nodes. All nodes are checked
for the package. Needs to be done once per Linux cluster.
.PP
.RS 2
# crm cluster run "zypper --non-interactive in -l -f -y SAPHanaSR-angi"
.br
# crm cluster run "hostname; rpm -q SAPHanaSR-angi --queryformat %{NAME}"
.RE
.PP
* Example for adding susHanaSR.py to sudoers.
.PP
Needs to be done on each node.
See manual page susHanaSR.py(7) and SAPHanaSR-hookHelper(8).
.PP
* Example for adding susHanaSR.py to global.ini and HANA.
.PP
Needs to be done for each HANA site.
See manual page susHanaSR.py(7) and SAPHanaSR-manageProvider(8).
.PP
* Example for adding angi SAPHanaController resource config to CIB.
.PP
Needs to be done once per Linux cluster.
See manual page ocf_suse_SAPHanaController(7), SAPHanaSR_basic_cluster(7) and
SUSE setup guides.
.PP
* Example for setting SAPHanaController resource out of maintenance.
.PP
First the SAPHanaController multi-state resource is refreshed, then it is set
out of maintenance. Name of the resource is mst_SAPHanaController_HA1_HDB00.
Of course status of cluster, HANA and system replication needs to be checked
before and after this action, see example above. Needs to be done once per
Linux cluster. See also manual page SAPHanaSR_maintenance_examples(7).
.br
Note: The srHook status for HANA secondary site migh be empty.
.PP
.RS 2
# crm resource refresh mst_SAPHanaController_HA1_HDB00
.br
# crm resource maintenance mst_SAPHanaController_HA1_HDB00 off
.RE
.PP
* Example for testing RA on secondary site and trigger susHanaSR.py.
.PP
This step is optional.
The secondary node is determined from SAPHanaSR-showAttr. On that node, the
hdbnameserver is killed. The cluster will recover the secondary HANA and set
the CIB attribute srHook.
Of course status of cluster, HANA and system replication needs to be checked.
.PP
.RS 2
# SECNOD=$(SAPHanaSR-showAttr --format script |\\
.br
awk -F"/" '$1=="0 Host"&&$3=="score=\"100\"" {print $2}')
.br
# echo $SECNOD
.br
# ssh root@$SECNOD "hostname; killall -9 hdbnameserver"
.RE
.PP
.\"
.SH FILES
.TP
/etc/sudoers.d/SAPHanaSR
recommended place for sudo permissions of HADR provider hook scripts
.TP
/hana/shared/$SID/global/hdb/custom/config/global.ini
on-disk representation of HANA global system configuration
.TP
/usr/share/SAPHanaSR/samples/global.ini
template for classical SAPHanaSR.py entry in global.ini
.TP
/usr/share/SAPHanaSR-angi/samples/global.ini_susHanaSR
template for susHanaSR.py entry in global.ini
.PP
.\"
.SH REQUIREMENTS
.PP
* OS, Linux cluster and HANA are matching requirements for SAPHanaSR, or SAPHanaSR-ScaleOut respectively, and SAPHanaSR-angi.
.br
* Linux cluster, HANA and system replication are in sane state before the upgrade. All cluster nodes are online.
.br
* The whole procedure is tested carefully and documented in detail before being applied on production.
.br
* Linux cluster, HANA and system replication are checked and in sane state before set back into production.
.PP
.\"
.SH BUGS
.br
In case of any problem, please use your favourite SAP support process to open a request for the component BC-OP-LNX-SUSE. Please report any other feedback and suggestions to feedback@suse.com.
.PP
.\"
.SH SEE ALSO
.br
\fBSAPHanaSR-angi\fP(7) , \fBSAPHanaSR\fP(7) ,
\fBocf_suse_SAPHana\fP(7) , \fBocf_suse_SAPHanaController\fP(7) ,
\fBSAPHanaSR.py\fP(7) , \fBsusHanaSR.py\fP(7) ,
\fBSAPHanaSR_maintenance_examples\fP(7) , \fBSAPHanaSR-showAttr\fP(8) ,
\fBcrm\fP(8) , \fBcrm_mon\fP(8) , \fBcrm_attribute\fP(8) , \fBcibadmin\fP(8) , 
.br
https://documentation.suse.com/sbp/sap/ ,
.br
https://www.suse.com/c/tag/towardszerodowntime/
.PP
.\"
.SH AUTHORS
.br
A.Briel, F.Herschel, L.Pinne.
.PP
.\"
.SH COPYRIGHT
.br
(c) 2024 SUSE LLC
.br
This maintenance examples are coming with ABSOLUTELY NO WARRANTY.
.br
For details see the GNU General Public License at
http://www.gnu.org/licenses/gpl.html
.\"
