.\" Version: 1.2 
.\"
.TH SAPHanaSR-tests-description_angi-BW 7 "12 Oct 2025" "" "SAPHanaSR-angi"
.\"
.SH NAME
.\"
SAPHanaSR-tests-description_angi-BW \- Functional tests for SAPHanaSR-angi scale-out BW.
.PP
.\"
.SH DESCRIPTION
.\"
Functional test are shipped for different scenarios. This tests could be run
out-of-the-box. The test cases are defined in dedicated files.
See manual page SAPHanaSR-tests-syntax(5) for syntax details. Tests for
SAPHanaSR-angi scale-out BW scenario are listed in
SAPHanaSR-tests-angi-ScaleOut_BW(7). 
.PP
Entry point for all predefined tests is a clean and idle Linux cluster and a
clean HANA pair in sync. 
See manual page SAPHanaSR_maintenance_examples(7) for detecting the correct
status and watching changes near real-time.
.PP
Each test can be executed by running the command SAPHanaSR-testCluster with
appropriate parameters. See manual page SAPHanaSR-testCluster(8).
.PP
Predefined functional tests specific for the SAPHanaSR-angi scale-out BW scenario:
.PP
\fBblock_prim_node_network\fP
.RS 2
Descr: Block all network at primary master node (not yet implemented).
.br
Topology: ScaleOut.
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: rcnetwork stop
.br
Expect: Primary master node fenced. Standby node promoted to master.
Rebooted node finally started as standby.
HANA stays online degraded and finally recovers to lss=3.
SR stays SOK.
No takeover. One fencing. One failover.
.br
Comment: Infrastructure failure, host auto-failover.
.RE
.PP
\fBblock_prim_site_network\fP
.RS 2
Descr: Block all network at primary site nodes (not yet implemented).
.br
Topology: ScaleOut.
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: rcnetwork stop 
.br
Expect: All primary nodes fenced and finally started as secondary.
HANA secondary becomes finally primary.
SR SFAIL and finally SOK.
One takeover. One fencing.
.br
Comment: Infrastructure failure, main cluster case.
.RE
.PP
\fBblock_secn_node_network\fP
.RS 2
Descr: Block all network at secondary master node (not yet implemented).
.br
Topology: ScaleUp, ScaleOut.
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: rcnetwork stop
.br
Expect: Secondary master node fenced. Standby node promoted to master.
Rebooted node finally started as standby.
HANA stays online degraded and finally recovers to lss=3.
No takeover. One fencing. One failover.
.br
Comment: Infrastructure failure, host auto-failover.
.RE
.PP
\fBblock_secn_site_network\fP
.RS 2
Descr: Block all network at secondary site nodes (not yet implemented).
.br
Topology: ScaleOut.
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: rcnetwork stop
.br
Expect: All secondary nodes fenced and finally restarted.
SR SFAIL and finally SOK.
No takeover. One fencing.
.br
Comment: Infrastructure failure, main cluster case.
.RE
.PP
\fBkill_prim_indexserver\fP
.RS 2
Desc: Kill primary master indexserver, for susChkSrv.py (not yet implemented).
.br
Topology: ScaleOut.
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Expect: Primary node stays online and finally started as standby.
Standby node promoted to master.
HANA stays online degraded and finally recovers to lss=3.
SR SFAIL and finally SOK.
No takeover. No fencing (action_on_lost=kill). One failover.
.br
Comment: Application failure, host auto-failover.
.RE
.PP
\fBkill_prim_node\fP
.RS 2
Descr: Kill primary master node (not yet implemented).
.br
Topology: ScaleOut.
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: systemctl reboot --force
.br
Expect: Primary master node fenced. Standby node promoted to master.
Rebooted node finally started as standby.
HANA stays online degraded and finally recovers to lss=3.
SR stays SOK.
No takeover. One fencing. One failover.
.br
Comment: Node failure, host auto-failover.
.RE
.PP
\fBkill_prim_site_nodes\fP
.RS 2
Descr: Kill all nodes of primary site (not yet implemented).
.br
Topology: ScaleOut.
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: systemctl reboot --force
.br
Expect: Primary master node and primary worker node fenced.
Both primary nodes finally started as secondary.
HANA primary finally started as secondary.
HANA secondary becomes finally primary.
SR SFAIL and finally SOK.
One takeover. One fencing.
.br
Comment: Node failure, main cluster case.
.RE
.PP
\fBkill_prim_standby_node\fP
.RS 2
Descr: Kill primary standby node (not yet implemented).
.br
Topology: ScaleOut.
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: systemctl reboot --force
.br
Expect: Primary standby node rebooted.
HANA stays online.
SR stays SOK.
No takeover. No fencing. No failover.
.br
Comment: Node failure, main cluster case.
.RE
.PP
\fBkill_prim_worker_node\fP
.RS 2
Descr: Kill primary worker node (not yet implemented).
.br
Topology: ScaleOut.
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: systemctl reboot --force
.br
Expect: Primary worker node fenced. Standby node takes over.
Rebooted node finally started as standby.
HANA stays online degraded and finally recovers to lss=3.
SR stays SOK.
No takeover. One fencing. One failover.
.br
Comment: Node failure, host auto-failover.
.RE
.PP
\fBkill_secn_node\fP
.RS 2
Descr: Kill secondary master node (not yet implemented).
.br
Topology: ScaleOut.
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: systemctl reboot --force
.br
Expect: Secondary master node fenced. Standby node promoted to master.
Rebooted node finally started as standby.
HANA stays online degraded and finally recovers to lss=3.
SR SFAIL and finally SOK.
One failover. One fencing. No takeover.
.br
Comment: Node failure, host auto-failover.
.RE
.PP
\fBkill_secn_site_nodes\fP
.RS 2
Descr: Kill all nodes at secondary site (not yet implemented).
.br
Topology: ScaleOut.
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: systemctl reboot --force
.br
Expect: All secondary nodes fenced, all finally online.
HANA primary stays online.
SR SFAIL and finally SOK.
No takeover. One fencing.
.br
Comment: Node failure, main cluster case.
.RE
.PP
\fBkill_secn_standby_node\fP
.RS 2
Descr: Kill secondary standby node (not yet implemented).
.br
Topology: ScaleOut.
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: systemctl reboot --force
.br
Expect: Secondary standby node rebooted.
HANA stays online.
SR stays SOK.
No takeover. No fencing. No failover.
.br
Comment: Node failure, main cluster case.
.RE
.PP
\fBkill_secn_worker_node\fP
.RS 2
Descr: Kill secondary worker node (not yet implemented).
.br
Topology: ScaleOut.
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: systemctl reboot --force
.br
Expect: Secondary worker node fenced. Standby node takes over.
Rebooted node finally started as standby.
HANA stays online degraded and finally recovers to lss=3.
SR SFAIL and finally SOK.
No takeover. One fencing. One failover.
.br
Comment: Node failure, host auto-failover.
.RE
.PP
Descriptions of functional tests not specific for the SAPHanaSR-angi scale-out
BW scenario can be found in manual page SAPHanaSR-tests-description(7):
.br
block_manual_takeover, block_sr, flup, free_log_area, kill_prim_ipaddr, 
kill_prim_pacemakerd, kill_prim_pacemkr-based, kill_prim_saphostexec,
kill_prim_worker_pacemakerd, 
kill_secn_pacemakerd, kill_secn_pacemkr-based, 
kill_secn_worker_pacemakerd, 
maintenance_cluster_hana_running, maintenance_cluster_turn_hana, 
maintenance_prim_supportconfig, nop,
restart_cluster_hana_running, restart_cluster, restart_cluster_turn_hana .
.PP
.\"
.\" .SH EXAMPLES
.\"
.\" TODO
.\" .PP
.\"
.SH FILES
.\"
.TP
/usr/share/SAPHanaSR-tester/json/angi-ScaleOut_BW/
functional tests for SAPHanaSR-angi scale-out BW scenarios.
.TP
/usr/bin/sct_test_*
shell scripts for un-easy tasks on the cluster nodes.
.PP
.\"
.SH REQUIREMENTS
.\"
See the REQUIREMENTS section in SAPHanaSR-tester(7) and SAPHanaSR-angi(7).
Of course, HANA database and Linux cluster have certain requirements.
Please refer to the product documentation.
.PP
Further there are requirements specific to the SAPHanaSR-angi scale-out BW style
scenario:
.PP
\fB*\fR For the SAPHanaSR-angi scale-out BW scenario, SAPHanaSR-tester currently
supports four nodes per site. Their configured roles are:
.br
"master1::worker:" (called master)
.br
"master2::standby:" (called standby)
.br
"master3::worker:" (called worcan)
.br
"worker::worker:" (called worker)
.\" TODO: standby: "standby::standby:"
.PP
\fB*\fR HANA database and Linux cluster are configured according to
.\" TODO: the SUSE setup guide for
the scale-out BW style scenario. 
.PP
\fB*\fR The shared storage or NFS is configured according to HANA storage API for
host auto-failover.
.PP
\fB*\fR No immediate fencing is configured.
.\" TODO: ? except for the SAPHanaFilesystem RA.
Particularly SAPHanaSR-alert-agent has not to be used.
.PP
\fB*\fR After each test, all nodes have to be set back to their configured role.
The landscape status of both sites has to be 4.
.PP
.\"
.SH BUGS
.\"
.\" In case of any problem, please use your favourite SAP support process to open
.\" a request for the component BC-OP-LNX-SUSE.
Please report any other feedback and suggestions to feedback@suse.com.
.PP
.\"
.SH SEE ALSO
.\"
\fBSAPHanaSR-tester\fP(7) , \fBSAPHanaSR-testCluster\fP(8) ,
\fBSAPHanaSR-tests-syntax\fP(5) , \fBSAPHanaSR-tests-description\fP(7) ,
\fBSAPHanaSR-tests-angi_BW\fP(7) ,
\fBSAPHanaSR-angi\fP(7) , \fBSAPHanaSR-showAttr\fP(8),
\fBocf_suse_SAPHanaController\fP(7) ,
.br
SAP note 1900823  - SAP HANA Storage Connector API ,
.br
SAP HANA Fiber Channel Storage Connector Admin Guide 
( https://www.sap.com/documents/2016/06/84ea994f-767c-0010-82c7-eda71af511fa.html )
.br
NetApp solutions for SAP storage configuration 
( https://docs.netapp.com/us-en/netapp-solutions-sap/bp/hana-aff-fc-io-stack-configuration.html ).
.\" TODO: https://docs.netapp.com/us-en/netapp-solutions-sap/bp/hana-aff-nfs-install-prep-nfsv4.html#sap-hana-hosts
.\" NFSv4 locks without LDAP etc.
.PP
.\"
.SH AUTHORS
.\"
F.Herschel, L.Pinne.
.PP
.\"
.SH COPYRIGHT
.\"
(c) 2025 SUSE LLC
.br
The package SAPHanaSR-tester comes with ABSOLUTELY NO WARRANTY.
.br
For details see the GNU General Public License at
http://www.gnu.org/licenses/gpl.html
.\"
