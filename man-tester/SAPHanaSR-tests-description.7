.\" Version: 1.001 
.\"
.TH SAPHanaSR-tests-description 7 "04 Jan 2024" "" "SAPHanaSR-angi"
.\"
.SH NAME
SAPHanaSR-tests-description \- Functional tests for SAPHanaSR.
.PP
.\"
.SH DESCRIPTION
.PP
Functional test are shipped for different scenarios. This tests could be run
out-of-the-box. The test cases are defined in dedicated files.
See manual page SAPHanaSR-tests-syntax(5) for syntax details. Tests for
SAPHanaSR-angi scale-up scenarios are listed in SAPHanaSR-tests-angi-ScaleUp(7),
for SAPHanaSR-angi scale-out ERP scenarios in SAPHanaSR-tests-angi-ScaleOut(7). 

Entry point for all predefined tests is a clean and idle Linux cluster and a
clean HANA pair in sync. Same is true for the final state. 
See manual page SAPHanaSR_maintenance_examples(7) for detecting the correct
status and watching changes near real-time.

Each test can be executed by running the command SAPHanaSR-testCluster with
appropriate parameters. See manual page SAPHanaSR-testCluster(8).
.PP
Predefined functional tests:
.PP
\fBblock_manual_takeover\fP
.RS 2
Descr: Blocked manual takeover, for susTkOver.py.
.br
Topology: ScaleUp, ScaleOut.
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: See susTkOver.py(7).
.br
Expect: Both nodes stay online.
Both HANA stay online.
Failed manual takeover attempt is logged in syslog and HANA tracefile.
SR stays SOK.
No takeover. No fencing.
.br
Comment: Admin mistake.
.RE
.PP
\fBblock_sr\fP
.RS 2
Descr: Block HANA SR and check SFAIL attribute; unblock to recover.
.br
Topology: ScaleUp, ScaleOut (not yet implemented).
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: See susHanaSR.py.(7).
.br
Expect: All nodes stay online.
Both HANA stay online.
SR SFAIL and finally SOK.
No takeover. No fencing.
.br
Comment: Infrastructure failure, main cluster case.
.RE
.PP
\fBblock_sr_and_freeze_prim_fs\fP
.RS 2
Descr: Block HANA SR and freeze HANA FS on primary node.
.br
Topology: ScaleUp (angi only).
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: See ocf_suse_SAPHanaFilesystem(7), susHanaSR.py.(7).
.br
Expect: Both nodes stay online.
HANA primary is stopped and finally back online.
SR SFAIL and finally SOK.
No takeover. No fencing.
.br
Comment: Infrastructure failure, main cluster case.
.RE
.PP
\fBblock_sr_and_freeze_prim_master_nfs\fP
.RS 2
Descr: Block HANA SR and freeze HANA NFS on primary master node
(not yet implemented).
.br
Topology: ScaleOut (angi only).
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: See ocf_suse_SAPHanaFilesystem(7), susHanaSR.py.(7).
.br
Expect: All nodes stay online.
HANA primary is stopped and finally back online.
SR SFAIL and finally SOK.
No takeover. No fencing.
.br
Comment: Infrastructure failure, main cluster case.
.RE
.PP
\fBblock_sr_and_freeze_prim_site_nfs\fP
.RS 2
Descr: Block HANA SR and freeze HANA NFS on primary site
(not yet implemented).
.br
Topology: ScaleOut (angi only).
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: See ocf_suse_SAPHanaFilesystem(7), susHanaSR.py.(7).
.br
Expect: All nodes stay online.
HANA primary is stopped and finally back online.
SR SFAIL and finally SOK.
No takeover. No fencing.
.br
Comment: Infrastructure failure, main cluster case.
.RE
.PP
\fBflup\fP
.RS 2
Descr: Like nop but very short sleep, just checking the test engine.
.br
Topology: ScaleUp, ScaleOut.
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: Wait and see.
.br
Expect: Cluster and HANA are up and running, all good.
.br
Comment: Just housekeeping.
.RE
.PP
\fBfree_log_area\fP
.RS 2
Descr: Free HANA log area on primary site.
.br
Topology: ScaleUp, ScaleOut.
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: Free up HANA transaction log space and log backups.
.br
Expect: Cluster and HANA are up and running, all good.
.br
Comment: Just housekeeping.
.RE
.PP
\fBfreeze_prim_fs\fP
.RS 2
Descr: Freeze HANA FS on primary node.
.br
Topology: ScaleUp (angi only).
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: See ocf_suse_SAPHanaFilesystem(7).
.br
Expect: Primary node fenced and finally started as secondary.
HANA primary stopped and finally started as secondary.
HANA secondary becomes finally primary.
SR SFAIL and finally SOK.
One takeover. One fence.
.br
Comment: Infrastructure failure, main cluster case.
.RE
.PP
\fBfreeze_prim_master_nfs\fP
.RS 2
Descr: Freeze HANA NFS on primary master node.
.br
Topology: ScaleOut (angi only).
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: See ocf_suse_SAPHanaFilesystem(7).
.br
Expect: Primary master node fenced, primary worker instance stopped.
HANA finally started as secondary.
HANA secondary becomes finally primary.
SR SFAIL and finally SOK.
One takeover. One fence. 
.br
Comment: Infrastructure failure, main cluster case.
.RE
.PP
\fBfreeze_prim_site_nfs\fP
.RS 2
Descr: Freeze HANA NFS on primary site.
.br
Topology: ScaleOut (angi only).
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: See ocf_suse_SAPHanaFilesystem(7).
.br
Expect: Primary master an worker node fenced.
HANA finally started as secondary.
HANA secondary becomes finally primary.
SR SFAIL and finally SOK.
One takeover. One fence. 
.br
Comment: Infrastructure failure, main cluster case.
.RE
.PP
\fBfreeze_secn_site_nfs\fP
.RS 2
Freeze HANA NFS on secondary site.
.br
Topology: ScaleOut (angi only).
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: See ocf_suse_SAPHanaFilesystem(7).
.br
Expect: Primary site keeps running.
HANA secondary site restarted.
SR SFAIL and finally SOK.
No takeover. No fence.
.br
Comment: Infrastructure failure, main cluster case.
.RE
.PP
\fBkill_prim_indexserver\fP
.RS 2
Descr: Kill primary indexserver, for susChkSrv.py.
On scale-out, kill primary master indexserver.
.br
Topology: ScaleUp, ScaleOut.
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: See susChkSrv.py(7).
.br
Expect: Primary node stays online.
HANA primary (master) stopped and finally started as secondary.
HANA secondary becomes finally primary.
SR SFAIL and finally SOK.
One takeover. No fencing (for action_on_lost=kill).
.br
Comment: Application failure, main cluster case.
.RE
.PP
\fBkill_prim_inst\fP
.RS 2
Descr: Kill primary instance.
On scale-out, kill primary master instance.
.br
Topology: ScaleUp, ScaleOut.
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: HDB kill
.br
Expect: Primary (master) node stays online.
HANA primary (master) stopped and finally started as secondary.
HANA secondary becomes finally primary.
SR SFAIL and finally SOK.
One takeover. No fencing.
.br
Comment: Application failure, main cluster case.
.RE
.PP
\fBkill_prim_node\fP
.RS 2
Descr: Kill primary node.
On scale-out, kill primary master node.
.br
Topology: ScaleUp, ScaleOut.
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: systemctl reboot --force
.br
Expect: Primary (master) node fenced and finally started as secondary.
HANA primary stopped and finally started as secondary.
HANA secondary becomes finally primary.
SR SFAIL and finally SOK.
One takeover. One fencing.
.br
Comment: Node failure, main cluster case.
.RE
.PP
\fBkill_prim_worker_indexserver\fP
.RS 2
Descr: Kill primary worker indexserver, for susChkSrv.py.
.br
Topology: ScaleOut.
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: See susChkSrv.py(7).
.br
Expect: HANA primary stopped and finally started as secondary.
HANA secondary becomes finally primary.
SR SFAIL and finally SOK.
One takeover. No fencing (for action_on_lost=kill).
.br
Comment: Application failure, main cluster case.
.RE
.PP
\fBkill_prim_worker_inst\fP
.RS 2
Descr: Kill primary worker instance.
.br
Topology: ScaleOut.
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: HDB kill
.br
Expect: HANA primary stopped and finally started as secondary.
HANA secondary becomes finally primary.
SR SFAIL and finally SOK.
One takeover. No fencing.
.br
Comment: Application failure, main cluster case.
.RE
.PP
\fBkill_prim_worker_node\fP
.RS 2
Descr: Kill primary worker node.
.br
Topology: ScaleOut.
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: systemctl reboot --force
.br
Expect: Primary worker node fenced. 
HANA primary stopped and finally started as secondary.
HANA secondary becomes finally primary.
SR SFAIL and finally SOK.
One takeover. One fencing.
.br
Comment: Node failure, main cluster case.
.RE
.PP
\fBkill_secn_indexserver\fP
.RS 2
Descr: Kill secondary indexserver, for susChkSrv.py.
On scale-out, kill secondary master indexserver.
.br
Topology: ScaleUp, ScaleOut.
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: See susChkSrv.py(7).
.br
Expect: HANA secondary stopped and finally online.
HANA primary stays online.
SR SFAIL and finally SOK.
No takeover. No fencing (for action_on_lost=kill).
.br
Comment: Application failure, main cluster case.
.RE
.PP
\fBkill_secn_inst\fP
.RS 2
Descr: Kill secondary instance.
On scale-out, kill secondary master instance.
.br
Topology: ScaleUp, ScaleOut.
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: HDB kill
.br
Expect: HANA secondary stopped and finally online.
HANA primary stays online.
SR SFAIL and finally SOK.
No takeover. No fencing.
.br
Comment: Application failure, main cluster case.
.RE
.PP
\fBkill_secn_node\fP
.RS 2
Descr: Kill secondary node.
On scale-out, kill secondary master node.
.br
Topology: ScaleUp, ScaleOut.
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: systemctl reboot --force
.br
Expect: Secondary (master) node fenced and finally online.
HANA primary stays online.
SR SFAIL and finally SOK.
No takeover. One fencing.
.br
Comment: Node failure, main cluster case.
.RE
.PP
\fBkill_secn_worker_inst\fP
.RS 2
Descr: Kill secondary worker instance.
.br
Topology: ScaleOut.
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test:
.br
Expect:

HANA primary stays online.
SR SFAIL and finally SOK.
No takeover. No fencing.
.br
Comment: Application failure, main cluster case.
.RE
.PP
\fBkill_secn_worker_node\fP
.RS 2
Descr: Kill secondary worker node.
.br
Topology: ScaleOut.
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: systemctl reboot --force
.br
Expect: Secondary worker node fenced and finally online.
HANA primary stays online.
SR SFAIL and finally SOK.
No takeover. One fencing.
.br
Comment: Node failure, main cluster case.
.RE
.PP
\fBmaintenance_cluster_hana_running\fP
.RS 2
Descr: Stop and restart cluster, keep HANA running.
.br
Topology: ScaleUp, ScaleOut.
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: See SAPHanaSR_maintenance_examples(7).
crm maintenance on;
crm cluster stop --all;
crm cluster start --all;
crm resource refresh <cln_topology>;
crm resource refresh <msl_controller>;
crm resource maintenance off;
.br
Expect: All nodes stay online.
Cluster stopped and restarted.
Both HANA keep running.
SR stays SOK.
No takeover. No fencing.
.br
Comment: Main admin procedure.
.RE
.PP
\fBmaintenance_cluster_turn_hana\fP
.RS 2
Descr: Maintenance procedure, manually turning HANA sites.
.br
Topology: ScaleUp, ScaleOut.
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: See SAPHanaSR_maintenance_examples(7), https://www.suse.com/c/sap-hana-maintenance-suse-clusters/ .
.br
Expect: All nodes stay online.
HANA primary stopped and finally started as secondary.
HANA secondary becomes finally primary by manual takeover.
SR SFAIL and finally SOK. 
One takeover. No takeover by cluster. No fencing.
.br
Comment: Main admin procedure.
.RE
.PP
\fBmaintenance_with_standby_nodes\fP
.RS 2
Descr: standby+online secondary then standby+online primary
.br
Topology: ScaleUp.
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: See SAPHanaSR_maintenance_examples(7).
.br
Expect: All nodes stay online.
HANA primary stopped and finally started as secondary.
HANA secondary becomes finally primary.
SR SFAIL and finally SOK.
One takeover. No fencing.
.br
Comment: Sub-optimal admin procedure.
.RE
.PP
\fBnop\fP
.RS 2
Descr: No operation - check, wait and check again (stability check).
.br
Topology: ScaleUp, ScaleOut.
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: Wait and see.
.br
Expect: Cluster and HANA are up and running, all good.
.br
Comment: Main cluster case.
.RE
.PP
\fBregister_prim_cold_hana\fP
.RS 2
Descr: Stop cluster, do manual takeover, leave former primary down and unregistered, start cluster.
.br
Topology: ScaleUp, ScaleOut (not yet implemented).
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test:
.br
Expect: All nodes stay online.
HANA primary stopped and finally started as secondary.
HANA secondary stopped and finally started as primary.
SR SFAIL and finally SOK.
One takeover. No takeover by cluster. No fencing.
.br
Comment: Admin mistake.
.RE
.PP
\fBrestart_cluster_hana_running\fP
.RS 2
Descr: Stop and restart cluster, keep HANA running.
.br
Topology: ScaleUp, ScaleOut (angi only).
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: crm maintenance on;
crm cluster stop --all;
crm cluster start --all;
crm resource refresh <cln_topology>;
crm resource refresh <msl_controller>;
crm resource maintenance off;
.br
Expect: All nodes stay online.
Cluster stopped and restarted.
Both HANA keep running.
SR stays SOK.
No takeover. No fencing.
.br
Comment: Sub-optimal admin procedure.
.RE
.PP
\fBrestart_cluster\fP
.RS 2
Descr: Stop and restart cluster and HANA.
.br
Topology: ScaleUp, ScaleOut (angi only).
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: crm cluster stop --all;
sapcontrol ... StartSystem;
sapcontrol ... StartSystem;
crm cluster start --all;
.br
Expect: All nodes stay online.
Cluster stopped and restarted.
Both HANA stopped and manually restarted.
SR SFAIL and finally SOK.
No takeover. No fencing.
.br
Comment: Sub-optimal admin procedure.
.RE
.PP
\fBrestart_cluster_turn_hana\fP
.RS 2
Descr: Stop cluster and HANA, manually start and takeover HANA, start cluster.
.br
Topology: ScaleUp, ScaleOut.
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: Stop cluster and HANA, manually start HANA and do takeover, restart cluster.
No resource maintenance, no resource refresh.
.br
Expect: All nodes stay online.
Both HANA stopped.
HANA primary finally started as secondary.
HANA secondary becomes finally primary by manual takeover.
SR SFAIL and finally SOK. 
One takeover. No takeover by cluster. No fencing.
.br
Comment: Sub-optimal admin procedure, challenge for susHanaSR.py.
.RE
.PP
\fBsplit_brain_prio\fP
.RS 2
Descr: Network split-brain with priority fencing.
.br
Topology: ScaleUp.
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: iptables -I INPUT -s <node> -j DROP
.br
Expect: Secondary node fenced and finally online.
Primary node stays online.
HANA primary stays online.
SR SFAIL and finally SOK.
No takeover. One fencing.
.br
Comment: Infrastructure failure, main cluster case.
.RE
.PP
\fBstandby_prim_node\fP
.RS 2
Descr: Set primary node standby and online again.
On scale-out, standby primary master node and online again.
.br
Topology: ScaleUp, ScaleOut.
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: crm node standby <node>; crm node online <node>
.br
Expect: All nodes stay online.
Primary (master) node standby and finally back online.
HANA primary stopped and finally started as secondary.
HANA secondary finally primary by takeover.
SR SFAIL and finally SOK.
One takeover. No fencing.
.br
Comment: Admin mistake on scale-out, sub-optimal procedure on scale-up.
.RE
.PP
\fBstandby_secn_node\fP
.RS 2
Descr: Set secondary node standby and online again.
On scale-out, standby secondary master node and online again.
.br
Topology: ScaleUp, ScaleOut.
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: crm node standby <node>; crm node online <node>
.br
Expect: Secondary (master) node standby and finally online.
HANA primary stays online.
HANA secondary stopped and finally started.
SR SFAIL and finally SOK. No takeover. No fencing.
.br
Comment: Admin mistake on scale-out, sub-optimal procedure on scale-up.
.RE
.PP
\fBstandby_secn_worker_node\fP
.RS 2
Descr: Set secondary worker node standby and online again.
.br
Topology: ScaleOut.
.br
Prereq: Cluster and HANA are up and running, all good.
.br
Test: crm node standby <node>; crm node online <node>
.br
Expect: Secondary worker node standby and finally online.
HANA primary stays online.
HANA secondary stays online. HANA worker clone_state goes to UNDEFINED and
finally to DEMOTED.
SR stays SOK. No takeover. No fencing.
.br
Comment: Admin mistake.
.RE
.PP
.\"
.SH EXAMPLES
.PP
* List all shipped tests
.PP
.RS 2
# find /usr/share/SAPHanaSR-tester/json/ -name "*.json" -exec basename {} \\; | sort -u
.RE
.PP
.\"
.SH FILES
.\"
.TP
/usr/share/SAPHanaSR-tester/json/angi-ScaleUp/
functional tests for SAPHanaSR-angi scale-up scenarios.
.TP
/usr/share/SAPHanaSR-tester/json/angi-ScaleOut/
functional tests for SAPHanaSR-angi scale-out ERP scenarios.
.TP
/usr/bin/sct_test_*
shell scripts for un-easy tasks on the cluster nodes.
.PP
.\"
.SH REQUIREMENTS
.\"
See the REQUIREMENTS section in SAPHanaSR-tester(7) and SAPHanaSR-angi(7).
Of course, HANA database and Linux cluster also have certain requirements.
Please refer to the product documentation.
.PP
.\"
.SH BUGS
In case of any problem, please use your favourite SAP support process to open
a request for the component BC-OP-LNX-SUSE.
Please report any other feedback and suggestions to feedback@suse.com.
.PP
.\"
.SH SEE ALSO
\fBSAPHanaSR-tester\fP(7) , \fBSAPHanaSR-testCluster\fP(8) ,
\fBSAPHanaSR-tests-syntax\fP(5) , \fBSAPHanaSR-tests-angi-ScaleUp\fP(7) ,
\fBSAPHanaSR-tests-angi-ScaleOut\fP(7) ,
\fBSAPHanaSR-tests-classic-ScaleUp\fP(7) ,
\fBSAPHanaSR-angi\fP(7) , \fBSAPHanaSR-showAttr\fP(8)
.PP
.\"
.SH AUTHORS
F.Herschel, L.Pinne.
.PP
.\"
.SH COPYRIGHT
(c) 2023-2024 SUSE LLC
.br
The package SAPHanaSR-tester comes with ABSOLUTELY NO WARRANTY.
.br
For details see the GNU General Public License at
http://www.gnu.org/licenses/gpl.html
.\"
